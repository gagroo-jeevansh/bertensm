{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cachedir = './cache'\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]=cachedir\n",
    "os.environ[\"HF_DATASETS_CACHE\"]=cachedir\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    AlbertForSequenceClassification,\n",
    "    AlbertTokenizer,\n",
    "    ElectraForSequenceClassification,\n",
    "    ElectraTokenizer\n",
    ")\n",
    "\n",
    "# Define the custom dataset class\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, csv_file, max_length, tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = self.data['hatespeech']  # Assuming 'hatespeech' is the column for labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['text'][idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        label = torch.tensor(self.labels[idx])  # Get the label for the corresponding text\n",
    "\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "path =  f\"./measuring_hate_speech.csv\"\n",
    "max_len = 128  # Define the maximum sequence length for BERT\n",
    "\n",
    "hate_speech_dataset = HateSpeechDataset(csv_file=path, max_length=max_len)\n",
    "\n",
    "batch_size = 32\n",
    "shuffle = True  # Set to True if the data should be shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_dict = {}\n",
    "\n",
    "bertok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "distilbertok = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "robertok = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "albertok = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "electratok = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "tok_dict['bert'] = bertok\n",
    "tok_dict['distilbert'] = distilbertok\n",
    "tok_dict['roberta'] = robertok\n",
    "tok_dict['albert'] = albertok\n",
    "tok_dict['electra'] = electratok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Fine-Tuning\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "hate_speech_dataset = HateSpeechDataset(\n",
    "    csv_file=path, \n",
    "    max_length=max_len, \n",
    "    tokenizer=tok_dict['bert']\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    hate_speech_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # type: ignore\n",
    "    args=training_args,                \n",
    "    train_dataset=hate_speech_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./model') # type: ignore\n",
    "tok_dict['bert'].save_pretrained('./model')\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained('./model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./model')\n",
    "\n",
    "# Inference\n",
    "text = \"I hate you\"\n",
    "inputs = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    truncation=True\n",
    ")\n",
    "input_ids = inputs['input_ids'].squeeze(0)\n",
    "attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "label = torch.tensor(1)  # Get the label for the corresponding text\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=label) # type: ignore\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "pred = torch.argmax(logits, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
